<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/myblog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=myblog/livereload" data-no-instant defer></script><meta charset="utf-8">
<base href="http://localhost:1313/myblog/">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>How Retrieval Augmented Generation Works | Sabrina&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="Introduction
Natural Language Processing (NLP) is the process of taking a natural language input and then processing this input into something that a computer can understand. Large Language Models, or LLMs, are a subfield of NLP that uses artificial intelligence to interpret and generate natural language; some popular examples include ChatGPT and Google Bard.
The disadvantage of LLMs is that they are trained on outdated information that might not be specific enough for a particular use case. Additionally, many can only accept up to a few thousand words, so it is impossible to pass in large documents. That is where Retrieval Augmented Generation comes in.">
<meta name="author" content="Sabrina Adler">
<link rel="canonical" href="http://localhost:1313/myblog/posts/how_rag_works/">
<link crossorigin="anonymous" href="http://localhost:1313/myblog/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/myblog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/myblog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/myblog/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/myblog/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/myblog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/myblog/posts/how_rag_works/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/myblog/" accesskey="h" title="Sabrina&#39;s Blog (Alt + H)">Sabrina&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/myblog/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/myblog/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/myblog/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/myblog/notes/" title="Course Materials">
                    <span>Course Materials</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      How Retrieval Augmented Generation Works
    </h1>
    <div class="post-meta"><span title='2025-07-16 00:00:00 +0000 UTC'>July 16, 2025</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Sabrina Adler

</div>
  </header> 
  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>Natural Language Processing (NLP) is the process of taking a natural language input and then processing this input into something that a computer can understand. Large Language Models, or LLMs, are a subfield of NLP that uses artificial intelligence to interpret and generate natural language; some popular examples include ChatGPT and Google Bard.</p>
<p>The disadvantage of LLMs is that they are trained on outdated information that might not be specific enough for a particular use case. Additionally, many can only accept up to a few thousand words, so it is impossible to pass in large documents. That is where Retrieval Augmented Generation comes in.</p>
<h2 id="definition-and-process">Definition and Process<a hidden class="anchor" aria-hidden="true" href="#definition-and-process">#</a></h2>
<p>Retrieval Augmented Generation (RAG) is a process that improves the outputs of large language models using context retrieved from a vector database. There are four main steps: first, it takes the documents (PDFs, text files, word documents) and embeds them into a vector; then, it takes these embeddings and stores them in a vector database; next, when the user asks a question, it finds chunks of text in the database that are similar to the question; finally, it uses this text to give the LLM extra context to answer the question.</p>
<h2 id="embedding-and-vectorization">Embedding and Vectorization<a hidden class="anchor" aria-hidden="true" href="#embedding-and-vectorization">#</a></h2>
<p>The user will select documents that are relevant to their use case. For example, if you want to ask an LLM about your car, you may upload the user manual. The next step is to chunk the document into chunks of text that can be embedded; typically, these chunk sizes are a sentence to a paragraph in length. These chunks of text are then passed to the embedding model.</p>
<p>An embedding model is a tiny LLM trained on how relevant words are to each other. Words or concepts that are more strongly related have closer vectors to each other. For example, the words “queen” and “chess” would have vectors that are very close to each other, but the word “lime” would have a very different vector from those two since they are not closely related. To understand how embeddings work better, the first five minutes of this <a href="https://www.youtube.com/watch?v=viZrOnJclY0&amp;list=PLblh5JKOoLUIxGDQs4LFFD--41Vzf-ME1&amp;index=18">video</a> explain how embeddings work in a very simplified way.</p>
<p>From the embedding model, it returns vectors of all of the chunks of text from the document that the user passed in. These vectors are then passed into a vector database, which stores the chunk number, the embedded vector, and the text in the chunk. Now, the document is entirely parsed and stored in a vector database so that it will be easier to pull the most relevant chunks.</p>
<h2 id="making-requests-to-the-vector-database">Making Requests to the Vector Database<a hidden class="anchor" aria-hidden="true" href="#making-requests-to-the-vector-database">#</a></h2>
<p>This step aims to get the chunks of text from the documents that the user passed in most relevant to the question. This step is important because the goal of a RAG pipeline is to supply the LLM with extra context to improve the answer. From the previous step, there is now a vector database that stores all of the chunks of text that have been embedded into vectors. Now, the user can query these documents.</p>
<p>The user can now enter a question, for example, “How many miles can I drive before changing my oil?”. The embedding model will then embed this question into a vector. It needs to embed the question into a vector because a mathematical calculation will occur between the vector of the question and all the other vectors of the chunks of text. From the embedding model, the closer two vectors are to each other, the more similar a meaning they have to each other. So, the point of the calculation is to find the vector of text with the smallest distance from the question, since the chunk of text will have the most similar meaning to the question.</p>
<p>Typically, the most common type of calculation is a semantic cosine similarity search which will return a value between negative one and one: one being the most similar and the negative one being the most opposite. Below is an example of the calculator it will perform, and the summations are to account for the number of dimensions that the vectors are in, typically anywhere from one hundred to one thousand.</p>
<p>This calculation will occur between all of the vectors, and only the text chunks with the vectors that have the highest similarity to the vector of the question will be returned. Continuing with the previous example, a chunk of text about the oil will have a higher cosine similarity score than a chunk of text about the windshield wipers; this means, the chunk of text about oil will get returned. Ideally, this calculation will get the chunks of text that will be the most relevant and useful for answering the question.</p>
<h2 id="passing-context-to-the-llm">Passing Context to the LLM<a hidden class="anchor" aria-hidden="true" href="#passing-context-to-the-llm">#</a></h2>
<p>The final step of the RAG pipeline is to pass the question alongside the retrieved context to an LLM to generate a final response. There are many different templates to format this, but typically they are something like: “You are a helpful assistant given the following context: {insert context retrieved}; Please answer this question only using the context provided: {insert user question}”.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>The RAG pipeline embeds the documents and stores them in a vector database. Then, the pipeline uses a cosine similarity search to find the most relevant chunks to the question. Finally, it sends the extra context and question to the LLM and gets a final answer. Retrieval Augmented Generation is a useful tool in artificial intelligence. It can make large language models better for a variety of use cases.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/myblog/">Sabrina&#39;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
